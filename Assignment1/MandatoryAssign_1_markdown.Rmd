---
title: "Mandatory Assignment 1"
author: "HÃ¥kon Berggren Olsen"
date: "Spring 2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: STK2100
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tinytex)


library(tidyverse)
library(glmnet)
```

# Problem 1.
The dataset, nuclear, contains data regarding the construction of 32 Light-Water Reactors built between 1967 and 1971. The scope of the dataset is to predict the cost of construction for further span given the dataset within this time interval. 

The dataset has 32 rows and 11 columns, and has a mix of continous- and categorical variabels. Contionous data such as regarding the cost, construction and net capacity of the a given nuclear powerplant, and categorical such as cooling tower present or absent, if there exists an LWR (Light-Water Reactor) plant at the same site and such. 

The description of all the variables are shown in a table below. 

| **Name**        	|                                                     **Continous Variables**                                                     	|
|------------------	|:-------------------------------------------------------------------------------------------------------------------------------:	|
| cost:            	| The capital cost of construction in millions of dollars adjusted to 1976 base.                                                  	|
| date:            	| The date on which the construction permit was issued. The data are measured in years since January 1 1990 to the nearest month. 	|
| t1:              	| The time between application for and issue of the construction permit.                                                          	|
| t2:              	| The time between issue of operating license and construction permit.                                                            	|
| cap:             	| The net capacity of the power plant (MWe).                                                                                      	|
| cum.n:           	| The cumulative number of power plants constructed by each architect-engineer.                                                   	|
| **Name** 	        |                                                    **Categorical Variables**                                                      |
|------------------	|---------------------------------------------------------------------------------------------------------------------------------	|
| pr:              	| A binary variable where 1 indicates the prior existence of a LWR plant at the same site.                                        	|
| ne:              	| A binary variable where 1 indicates that the plant was constructed in the north-east region of the U.S.A.                       	|
| ct:              	| A binary variable where 1 indicates the use of a cooling tower in the plant.                                                    	|
| bw:              	| A binary variable where 1 indicates that the nuclear steam supply system was manufactured by Babcock-Wilcox.                    	|
| pt:              	| A binary variable where 1 indicates those plants with partial turnkey guarantees.                                               	|

## a) Loading the dataset


```{r cars}
datadir = "http://www.uio.no/studier/emner/matnat/math/STK2100/data/"
nuclear = read.table(paste(datadir, "nuclear.dat", sep=""), header=T)
```

Generating some plots to gain an intuition for the dataset.

```{r}

plot(nuclear)
```
This is very cluttered and hard to grasp, much due to categorical variables not giving any immeadiate insight. Therefore we will plot the dataset only regarding the continous variables.

```{r}
cont_var <- c("cost", "date", "t1", "t2", "cap", "cum.n")
plot(nuclear[cont_var])
```
\

From this figure we can see some clear trends. We can see the cost of the LWRs are increasing over time, which can be attributed to inflation. A very positive correlation is also spotted in the t1 vs date indicating that the time between application and issue of the construction permits is steadily increasing and almost doubles at the end of the dataset. t2 vs date shows however a negative correlation, indicating that it takes shorter and shorter time from obtaining operating license to gaining construction permit.

The net capacity of the LWR, cap, does show a slight increase with cost. This is expected as the capital cost of the investment should match it's returns.


### Box plots for the categorical variables

```{r}
# Box plots
cat_var <- c("pr", "ne", "ct", "bw", "pt")

boxplot(nuclear[cat_var])
```



## b) Constructing a model

Look at the model 

$$ Y_i = \beta_0 +\beta_1 x_{i,1}+ \cdot\cdot\cdot +\beta_p x_{p,i} + \epsilon_i $$

where $Y_i$ is cost at log scale for observation $i$. 

The standard assumptions about the noise term $\epsilon_i$ are that it is an independent, identically distributed, random variable from a Gaussian distribution $N(0,\sigma^2)$. It also assumed that the variance of the noise is constant $\text{Var}[\epsilon_i] =\sigma^2$ and that the expectance is $\mathbb{E}[\epsilon_i]=0$. 
 
```{r}
#fit with all covariates
model2 = lm(log(cost)~., data=nuclear)
summary(model2)
```

The model on the log-transformed cost shows an $R^2$ value of $0.799$. Although there are alot of covariates which have high p-values and therefore does not help the model with more accurate predictions. As seen from the plots in problem a), the net capacity has a strong, linear relationship with the capital cost of the power plant.

## c) Removing covariates with high P-value

Removing the highest corresponding P-value, i.e. t1 gives a better result as a high P-value means that the outcomes in $log(cost)$ cannot be sufficiently explained by the changes in $t1$. When doing this we get

```{r}
#Fit with t1 taken away.
model2 = lm(log(cost)~.-t1,data=nuclear)
summary(model2)
```
 
 An increase from 13.28 to 15.42 in the F-statistic (explain F-statistic), increase of the multiple R-squared from 0.8635 to 0.8631 and likewise 0.7985 to 0.8072 for the adjusted R-squared. 
 
## d) Filtering on p-values
Iterating over all the covariates to remove those that do not have a p-value over 0.05.

```{r}
# Iteratively removing the highest covariate with p-value over 0.05
variables <- names(nuclear)[-1]
target <- "log(cost)"

removed_variables = c("-t1")
for (i in 1:(length(variables)-1)){
  # Defining the formula
  f <- as.formula(
    paste(target, paste(removed_variables,
                        collapse= " - "),
                        sep = " ~."))
  
  # Computing the model
  filtermodel <- lm(f, data=nuclear)

  # Saving R2 coefficients
  
  # Finding the covariate with the highest p value
  p_values <- summary(filtermodel)$coefficients[2:(length(variables)-length(removed_variables)),4]
  
  max_p_ind <- which.max(p_values)
  max_p_name <- names(p_values[max_p_ind])
  
  # Checking if the highest p value is higher than 0.05
  if (p_values[max_p_ind]>0.05){
    removed_variables <- append(removed_variables, max_p_name)
  
  } else {
    break
  }
}

summary(filtermodel)
```


```{r}
dev.new(width=5, height=5, unit="cm")

plot(filtermodel)
```

Normal Q-Q plot indicates some datapoints as outliers for a normally distributed assumption.

Scale Location plot shows that the variance of the residuals are somewhat constant, however with an increase towards higher fitted values.

Residuals vs Leverage plot shows very horizontal trend, although with two datapoints exhibiting high leverage.




## e) Model based on Mean Squared Error

Evaluating the model with Mean Squared Error $(\frac{1}{n} \sum_{i=1}^n(y_i-\hat{y_i})^2)$:

```{r}
y.hat <- predict(filtermodel, nuclear)
y <- log(nuclear$cost)

MSE <- sum((y-y.hat)^2)/length(y)
MSE
```
Evaluating the model with the average quadratic error we get on average a squared difference of $0.026$. But this does not tell us everything about the model. Additionally, computing the $R^2$ metric gives us a indication of how the variance in the data is transferred to the model. 

```{r}
R2 <- 1- sum((y-y.hat)^2)/sum((y-mean(y))^2)
R2
# Or:
var(y.hat)/var(y)
```
With a $R^2$ of $0.809$ which tells us that the model explains $80.9%$ of the variance in data.



## f) Differences between confidence and predict commands

Given a new datapoint $\mathbf{x*}$, we want to find out the $\theta = \mathbb{E}[Y|\mathbf{x^*}]$ as well as $\eta = \mathbb{E}[\text{exp}(Y)|\mathbf{x^*}]$. 

Predicting the datapoint from the problem with two different intervals: confidence and predict
```{r}

d.new <- data.frame(date=70.0, t1=13, t2=50, cap=800, pr=1, ne=0, ct=0, bw=1, cum.n=8, pt=1)

interval.conf <- predict(filtermodel, d.new, interval="confidence")
interval.pred <-predict(filtermodel, d.new, interval="predict")

interval.conf
interval.pred

```
For both the interval choices, the fit is the same. However the confidence intervals upper and lower bounds are narrower than the prediction. 

The confidence interval portrays the uncertainty regarding the mean of the prediction values, and as $\mathbb{E}[\epsilon]=0$, the random, gaussian noise gets omitted from the calculations. The prediction interval however portrays the uncertainty of a single, predicted point. Therefore it takes into account the random noise $\epsilon$.


## g) Constructing log intervals on non-logarithmic scale
Plotting the intervals from the previous problem onto non-logscale:

```{r}

CI <- data.frame(rbind(interval.conf, interval.pred))
CIexp <- exp(CI)
CIexp
ggplot(data=CIexp, aes(x=seq(nrow(CIexp)), y=fit))+
  geom_point() + 
  geom_errorbar(aes(ymin = lwr, ymax = upr))
```



## h) Lasso regression

Lasso regression on the data set is shown below.

```{r}
# Lasso regression
library(glmnet)

grid <- 10^seq(5, -2, length=100)

x <- nuclear[ ,-1]
y <- log(nuclear[ , 1])

# Fitting with Lasso Regression : alpha=1 equals the l1 norm
lasso.model <- glmnet(x,y, alpha=1, lambda=grid)

#Plotting estimates for different values of lambda
plot(lasso.model, xvar="lambda")

#Dividing into training/test set
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)

x.train = as.matrix(x[train,])
x.test = as.matrix(x[test,])
y.train = y[train]
y.test=y[test]

#Cross-validation
set.seed(1)
cv.out <- cv.glmnet(x.train, y.train, alpha=1)
plot(cv.out)
```
After cross-validation for the selection of the penalty parameter, the variables included in the final model are everyone expect ct and bw, as shown below. Comparing with the previous model, the lasso regression has a much lower intercept but has not omitted as many covariates.

```{r}
coef(cv.out, cv.out$lambda.min)
filtermodel$coefficients
```




# Problem 2.


## a) Reading data

Getting the data
```{r}
datadir = "http://www.uio.no/studier/emner/matnat/math/STK2100/data/"
Fe <- read.table(paste(datadir, "fe.dat", sep=""), header=T, sep=",")
```


Running the code from the problem with contrasts:

```{r}
options(contrasts=c("contr.treatment", "contr.treatment"))

fit1 <- lm(Fe ~ form, data=Fe)
summary(fit1)

```

This model behaves poorly with an $R^2$ of around $0.364$. The model only assumes there is one covariate, which is form, and that it ranges between 1,2,3 and 4.

Transforming the "form" column into factors. This lets the model know there are four different covariates, denoted by the dummy variables "form"


```{r}
Fe$form <- as.factor(Fe$form)
fit1 <- lm(Fe~form, data=Fe)
summary(fit1)
```
The resulting $R^2$ has now improved up to $0.431$.


## b) Constraints
As explained in the text, the "options(contrasts=c("contr.treatment", "contr.treatment"))" code will set the constraint that $\beta_1=\hat{\beta_1}=0$.

This is needed as to portray the relative differences of iron content between the different formations, without discarding the intercept. The interpretation of the other $\beta_j$ parameters is therefore how much more (or less, if negative) iron are in the formations relative to formation 1 (carbonates).



## c) Alternative Constraint

An alternative constraint is to put $\beta_0 = 0$, i.e. removing the intercept. This can be obtained by 
```{r}

fit2 <- lm(Fe~form+0, data=Fe)

summary(fit2)

```

By setting the intercept, $\beta_0=0$, the $\beta_j$ is then interpreted as the mean of the iron content in their respective formations. 


## d) Sum of coefficients equals zero
Using the "contrast=c("contr.sum", "contr.sum")" imposes the constraint $\sum_{j=1}^K \beta_j = 0$. However, as the summary of the fit only displays three of the $\beta_j$, the last one can be found by taking the negative of the sum of all the coeffiecients expect the intercept.
```{r}
options(contrasts=c("contr.sum", "contr.sum"))
fit3 <- lm(Fe~form, data=Fe)
summary(fit3)

beta4 <- -sum(fit3$coefficients[2:4])

```


## e) Differences between formations 
We now have three different models which all are based on constraints on some or more of the $\beta_j$. Printing out the coefficients for all the different models gives us the following 


```{r}
print("Model 1 - First Coeff Zero")
model1 <- fit1$coefficients[2:4]
intercept <- fit1$coefficients[1]
print(setNames(c(intercept, 0,model1), c("(intercept)","form1", names(model1))))

print("Model 2 - no intercept")
model2 <- fit2$coefficients
print(setNames(c(0,model2), c("(intercept)", names(model2))))


print("Model 3 - sum of coeffs zero ")
model3 <- fit3$coefficients
print(setNames(c(model3, beta4),c(names(model3), "form4")))


```
The different models are different combinations of the $\beta_j$. The first model shows the intercept as the iron content of carbonate (form1), while the other coefficients point to how much mure iron there are in the differing formations.

The second model just shows the average iron content across all the formation.

The third model shows the average difference in iron in the formation relative to the average iron in the entire dataset. 

As to which model is better to convey that there exists iron differences, it is somewhat subjective, however I would prefer the third model.

## f) Predicting on four datapoitns
Creating four new datapoints and predicting upon them
```{r}


newdata = data.frame(form=as.factor(c(1,2,3,4)))

newdata

pred1 = predict(fit1, newdata)
pred2 = predict(fit2, newdata)
pred3 = predict(fit3, newdata)

pred1
pred2
pred3

```

All these predictions are the same, as the models are essentially similiar as discussed in the previous section, and only differs by how large the intercept is.

## g) Summary ouputs of the model
The summary outputs of the different models are shown below. As the second model which omits an intercept gives the best $R^2$ score, it should suffice to simplify the model to this extent i.e. basing the model only on the means of the iron content for the respecitve formations.

```{r}
summary(fit1)

summary(fit2)

summary(fit3)
```






